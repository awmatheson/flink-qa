{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Document Processing and Question Answering System\n",
    "\n",
    "This notebook demonstrates how to use the PDF processing and question answering system in an interactive environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apache-flink==2.0.0 apache-flink-libraries==2.0.0 PyPDF2==3.0.1 sentence-transformers==2.2.2 huggingface-hub==0.16.4 faiss-cpu==1.7.4 numpy==1.24.3 pandas==2.0.3 transformers==4.30.2 torch==2.1.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flink_processor import FlinkProcessor\n",
    "from vector_store import VectorStore\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Flink processor\n",
    "flink_processor = FlinkProcessor()\n",
    "\n",
    "# Initialize vector store\n",
    "vector_store = VectorStore()\n",
    "\n",
    "# Initialize question answering model\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "print(\"Components initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing PDF documents\n",
    "pdf_directory = \"docs\"\n",
    "\n",
    "# Process documents using Flink\n",
    "processed_docs = flink_processor.process_directory(pdf_directory)\n",
    "print(f\"Processed {len(processed_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Documents in Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store processed documents\n",
    "for doc in processed_docs:\n",
    "    vector_store.add_document(\n",
    "        filename=doc['filename'],\n",
    "        content=doc['content'],\n",
    "        embedding=doc['embedding']\n",
    "    )\n",
    "print(\"Documents stored in vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, top_k: int = 3) -> str:\n",
    "    \"\"\"Answer a question using the stored documents and LLM.\"\"\"\n",
    "    # Get relevant documents\n",
    "    relevant_docs = vector_store.search(question, top_k=top_k)\n",
    "    \n",
    "    # Prepare context from relevant documents\n",
    "    context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc['content']}\" \n",
    "                             for i, doc in enumerate(relevant_docs)])\n",
    "    \n",
    "    # Prepare prompt for the model\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    \n",
    "    # Generate answer\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=200,\n",
    "        num_beams=4,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Print relevant documents for reference\n",
    "    print(\"\\nBased on the following documents:\")\n",
    "    for i, doc in enumerate(relevant_docs):\n",
    "        print(f\"\\nDocument {i+1} (from {doc['filename']}):\")\n",
    "        print(f\"Relevance: {doc['relevance']} matching chunks, average score: {doc['score']}\")\n",
    "        print(doc['content'][:500] + \"...\")\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example questions\n",
    "questions = [\n",
    "    \"What are the key requirements for the system?\",\n",
    "    \"What are the main features of the architecture?\",\n",
    "    \"How does the system handle document processing?\"\n",
    "]\n",
    "\n",
    "# Answer each question\n",
    "for question in questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    answer = answer_question(question)\n",
    "    print(f\"\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Questions\n",
    "\n",
    "You can ask your own questions by calling the `answer_question` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own question\n",
    "your_question = \"What are the similarities in requirements?\"\n",
    "answer = answer_question(your_question)\n",
    "print(f\"\\nQuestion: {your_question}\")\n",
    "print(f\"\\nAnswer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
